
## ðŸ“˜ Apache Spark & HDFS Quiz Questions with Explanations

---

### âœ… QUESTION 1: What is one of the main benefits of Spark's in-memory processing?

â—»Â  Reduced CPU usage  
âœ… Reduced I/O operations and faster processing  
â—»Â  Fewer cluster nodes required  
â—»Â  Increased disk storage utilization  

#### ðŸ’¡ Explanation:
Spark keeps intermediate data in **memory (RAM)** rather than writing to disk, which:  
- Minimizes **disk I/O operations**  
- Speeds up data processing significantly  
This is particularly useful in iterative machine learning and data analytics tasks.

---

### âœ… QUESTION 2: Which of the following languages is NOT supported by Apache Spark?

âœ… JavaScript  
â—»Â  Python  
â—»Â  Scala  
â—»Â  Java  

#### ðŸ’¡ Explanation:
Apache Spark supports **Scala, Java, Python, R, and SQL**, but **not JavaScript**.  
JavaScript is primarily a web development language and not suitable for Sparkâ€™s distributed computing model.

---

### âœ… QUESTION 3: Which characteristic of Apache Spark allows it to handle both static and dynamic data?

â—»Â  Dependency on Hadoop  
â—»Â  High reliance on disk storage  
âœ… Support for hybrid workloads  
â—»Â  Sequential data processing  

#### ðŸ’¡ Explanation:
Spark supports **hybrid workloads**, meaning it can process both **batch (static)** and **streaming (dynamic)** data using the same engine, enabling seamless real-time and historical analysis.

---

### âœ… QUESTION 4: What is the primary role of the driver in Spark?

â—»Â  Storing large datasets across clusters  
â—»Â  Performing data computation  
â—»Â  Monitoring worker node health  
âœ… Coordinating and scheduling tasks  

#### ðŸ’¡ Explanation:
The **driver** is the brain of a Spark application. It:  
- Breaks the job into **tasks**  
- Sends them to **executors**  
- **Monitors** the overall execution and manages resources

---

### âœ… QUESTION 5: Which component divides a Spark job into smaller tasks?

â—»Â  Resource Manager  
â—»Â  Executor  
âœ… Driver  
â—»Â  Worker Node  

#### ðŸ’¡ Explanation:
The **Driver** analyzes the job and splits it into **stages and tasks**, assigning them to **executors** for parallel execution.

---

### âœ… QUESTION 6: Where does Spark temporarily store intermediate data for faster computation?

âœ… In-memory caching of executors  
â—»Â  In the driver  
â—»Â  On disk  
â—»Â  Within the Resource Manager  

#### ðŸ’¡ Explanation:
Spark uses **in-memory caching** within **executors** to hold intermediate results, which **avoids repeated computations** and **speeds up performance**.

---

### âœ… QUESTION 7: Which of the following best describes Spark's fault-tolerant feature?

â—»Â  It eliminates the need for backups.  
â—»Â  It relies solely on driver memory.  
â—»Â  It only prevents node failures.  
âœ… It tracks computation lineage to recover from failures.  

#### ðŸ’¡ Explanation:
Spark maintains a **lineage graph** of transformations, allowing it to **recompute lost data** if a failure occurs, instead of restarting the entire job.

---

### âœ… QUESTION 8: What is the purpose of lineage information in RDDs?

âœ… To recover data in case of failures  
â—»Â  To speed up data processing  
â—»Â  To store replicas of RDDs for backup  
â—»Â  To transform RDDs into DataFrames  

#### ðŸ’¡ Explanation:
**Lineage** in RDDs keeps a **record of all transformations** used to derive them. This allows Spark to **rebuild lost partitions** in case of failure.

---

### âœ… QUESTION 9: What differentiates wide transformations from narrow transformations?

âœ… Wide transformations require data shuffling across partitions.  
â—»Â  Wide transformations modify the original dataset.  
â—»Â  Wide transformations execute immediately.  
â—»Â  Wide transformations do not support lazy evaluation.  

#### ðŸ’¡ Explanation:
**Wide transformations** involve **data shuffling** between partitions and are more expensive. Examples include `groupByKey`, `reduceByKey`, and `join`.

---

### âœ… QUESTION 10: What is the purpose of lazy evaluation in RDDs?

â—»Â  To immediately execute transformations  
âœ… To build an optimized execution plan before processing data  
â—»Â  To reduce memory consumption by avoiding caching  
â—»Â  To allow real-time data processing  

#### ðŸ’¡ Explanation:
Lazy evaluation allows Spark to **delay execution** of transformations until an **action** is called. This helps in **optimizing the execution plan** and avoiding unnecessary computations.

---

